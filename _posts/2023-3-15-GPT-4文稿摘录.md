# 1 引言

大家好，我是渐构社群的 YJango。

如题，这个视频是关于 ChatGPT 的，一个足以影响整个人类社会的技术。

但它没有对国内开放，通用媒体又缺少相应的知识，所以我觉得我有义务做一个视频，向普通大众全面科普一下 ChatGPT 的原理，并提供一个视角让大家意识到它为何如此重要，让那些没有机会了解这件事的人也能应对即将到来的变革。

接下来，我将抛开技术细节，少用专业名词，在整体功能上讲解 ChatGPT 的「工作原理」「制造过程」「涌现的能力」「未来的影响」以及「如何应对」。让大家明白：

- ChatGPT是如何回答问题的
- 它是怎么被制造的，为什么它不是搜索引擎
- 它有哪些惊人能力，为什么它不只是聊天机器人
- 它将给社会带来什么样的冲击
- 我们该如何维持未来的竞争力

# 2 底层原理

首先是这项技术的底层原理。视频将逐一介绍它的「实质功能」「训练方式」「长板」和「短板」。

## 2.1 实质功能

尽管 ChatGPT 展现出的能力很惊人，但它也没有大众想得那么神，它没有意识，没有欲望，没有情绪，甚至都不理解自己说了什么，就像一只会学话的鹦鹉。

ChatGPT的实质功能非常简单，四个字就能概括：「单字接龙」。具体来说就是：给它「任意长的上文」，它会用「自己的模型」去生成「下一个字」。

例如，当给它「“我”」这个上文时，它可能会生成「“是”」。

当给它「“我是”」这个上文时，它可能会生成「“一”」。

ChatGPT本身能做的就只有「生成下一个字」，你所看到的回答全都是用「同一个模型」，根据不同的「上文」生成出来的。

那它是怎么回答那些长内容的呢？

答案是：把它自己生成的「下一个字」和「之前的上文」组合成「新的上文」，再让它以此生成「下一个字」。不断重复，就可以生成「任意长的下文」了。该过程也叫“自回归生成”。

例如，当它根据「“我是”」生成了「“一”」之后，把新生成的「“一”」和之前的「“我是”」组合成新的上文，再让它计算「“我是一”」后面接什么字。

假设这次它生成的是「“只”」，那再把「“只”」和「“我是一”」组合起来，让它计算「“我是一只”」后面接什么字。不断重复，就能生成“我是一只小小鸟”了。

## 2.2 训练方式

影响ChatGPT生成结果的因素主要有两个：除了「上文」外，另一个就是它的「模型」本身。「模型」就相当于「ChatGPT的大脑」。即使把同一个上文，送给不同的模型，也会生成不同的结果。

就好比这两只鹦鹉，同样是听到「“我”」这个上文，一只会接「“是”」，而另一只会接「“爱”」，因为两只鸟的主人，一人教的是《我是一只小小鸟》，另一人教的是《我爱你中国》。

为了让「ChatGPT」生成我们想要的结果，而非胡乱生成，就要提前训练「ChatGPT的大脑」，也就是训练它的模型。

训练方式是：让它遵照所给的「学习材料」来做单字接龙。通过不断调整模型，使得给模型「学习材料的上文」后，模型能生成「对应的下一个字」。

例如，当我们想把《登鹳雀楼》作为「学习材料」来训练ChatGPT时，就不断调整「它的模型」，使得：

- 给它「“白”」，它能生成「“日”」

- 给它「“白日”」，它能生成「“依”」

- 给它「“白日依”」，它能生成「“山”」

  …

- 一直到，给它「“白日依山尽，黄河入海流。欲穷千里目，更上一层”」，它能生成「“楼”」。

没学习前，它原本会胡乱生成，但学习后就可以在看到“白日依山尽”时，生成“黄河入海流”了。

那如果同时训练了「“白日依山尽，黄河入海流”」和「“白日何短短，百年苦易满”」，再遇到「“白日”」时，会怎么生成「下一个字」？

答案是：按照概率来抽样。有可能会生成「“依”」，也有可能生成「“何”」。

实际上 ChatGPT 给出的结果长这样，也就是「所有字的概率分布」，「生成的下一个字」就是按照概率分布抽样得到的结果。

由于抽样结果具有随机性，所以 ChatGPT 的回答并不是每次都一样。

不过，这样训练后，无非就是能补全和续写，那ChatGPT又是怎么回答问题的呢？

其实仅靠单字接龙就能回答问题。因为提问和回答也都是文字，可以将二者组合成一个「问答范例（学习材料）」让ChatGPT做单字接龙。

例如，当我们想让ChatGPT学习「怎么回答白日依山尽的下一句」时，就可以把「这个提问」和「正确回答」组合成一个「问答范例（学习材料）」，让它按照“请问白日依山尽的下一句是什么 白日依山尽的下一句是黄河入海流”来做单字接龙。

这样以来，当用户输入“请问白日依山尽的下一句是什么”时，它就能生成“白日依山尽的下一句是黄河入海流”了。

## 2.3 长板

但提问和回答的方式无穷无尽，像上面的提问还可以是：

- “白日依山尽的下一句？”
- “白日依山尽的后续内容是？”
- “告诉我白日依山尽的后续”

难道说要把所有的「提问-回答组合」都给ChatGPT来做单字接龙吗？

其实不需要。因为训练的主要目的不是记忆，而是学习。

以「单字接龙」的方式来训练「模型」，并不仅仅是为了让「模型」记住某个提问和回答。毕竟在训练之前，数据库已经将所有信息都记忆好了，直接搜索就可以得到答案，没必要先将信息从数据库移动到模型中，再让模型来生成。

之所以不直接搜索，非要训练单字接龙，为的就是让「模型」学习「提问和回答的通用规律」，以便在遇到「从没记忆过的提问」时，也能利用「所学的规律」生成「用户想要的回答」，这种举一反三的目标也叫「泛化」。

例如，当我们用这三个「学习材料」训练 ChatGPT 做单字接龙时：

- “白日依山尽的下一句？**白**日依山尽的下一句是黄河入海流”
- “白日依山尽的后续内容是？**白**日依山尽的下一句是黄河入海流”
- “告诉我白日依山尽的后续 **白**日依山尽的下一句是黄河入海流”

不论面对哪个提问，ChatGPT都会被要求生成“白日依山尽的下一句是黄河入海流”，这会驱使 ChatGPT 去建构三个提问的通用规律，将自己的模型调整为适用于三个提问的通用模型。

经过这种训练后，即使ChatGPT遇到“写出‘白日依山尽’的下一句”这种没记忆过的提问时，依靠「学习后的模型」，就有可能举一反三，也生成“白日依山尽的下一句是黄河入海流”。

很多人都会错误地认为，ChatGPT是“搜索引擎的升级版本”，是在庞大的数据库中通过超高的运算速度找到最接近的内容，然后进行一些比对和拼接，最终给出结果。

但实际上，ChatGPT 并不具备那种搜索能力。因为在训练过程中，学习材料并不会被保存在模型中，学习材料的作用只是「调整模型」以得到「通用模型」，为的是能处理「未被数据库记忆的情况」。所有结果都是通过「所学到的模型」根据上文，逐字生成的，因此 ChatGPT 也被称为「生成模型」。

「生成模型」与「搜索引擎」非常不同，搜索引擎无法给出「没被数据库记忆的信息」，但生成语言模型可以，还能创造不存在的文本，这正是它的长板，但它却有一些搜索引擎没有的短板。

## 2.4 短板

首先就是：搜索引擎不会混淆记忆，但它有可能会。

为了能应对「未被记忆的情况」，它会学习语言单位（如单词、短语、句子等）之间的规律，用「学到的规律」来成回答，然而，这也意味着如果出现了「实际不同但碰巧符合同一个规律」的内容，模型就可能混淆它。

最直接的结果是：若「现实中不存在内容」刚好符合「它从训练材料中学到的规律」，那 ChatGPT 就有可能对「不存在内容」进行「合乎规律的混合捏造」。

例如，我问它「“三体人为什么害怕大脸猫的威慑，62年都不敢殖民地球？”」，这个问题并不存在，但又刚好符合「它曾训练过的科幻材料中的规律」，于是它就用「科幻材料中所学到的规律」开始混合捏造了。

这也是为什么，当有人问它事实性内容时，可能会看到它胡说八道。

另一个问题是：它的内容无法被直接增删改查。

不论是 ChatGPT 「所记住的信息」，还是「所学到的规律」，都是以同一个模型的形式来表达的，因此我们无法像操作数据库那样，对这些内容直接进行增删改查。

这会导致两个具体问题：

第一：由于我们很难理解它所建构的规律，又无法直接查看它记住了什么、学到了什么，只能通过多次提问来评估和猜测它的所记所学，其决策缺乏可解释性，这难免会在使用时带来安全风险。

第二：由于只能通过再次调整模型（即再次训练）来增加、删除或修改它的所记所学，这难免在更新时会降低效率。

例如，对于「它编造[大脸猫](https://www.modevol.com/episode/clf9d5kni0zo301mm6tkl9t87#大脸猫威慑)」的毛病，无法通过直接修改它的回答来矫正，只能通过再训练它做「“三体人为什么害怕大脸猫的威慑？三体人和大脸猫无关”」的单字接龙来调整模型。可这样调整后的效果如何、是否会矫枉过正，或是引入其他问题，又得通过多次提问来评估，容易顾此失彼，效率低下。

还有个特点是：ChatGPT高度依赖数据，也就是学习材料。

想要让ChatGPT能够应对无数未见情况，就必须提供数量足够多、种类足够丰富，质量足够高的学习材料，否则它将无法学到通用规律，给出的回答将会是以偏概全的。

此外，ChatGPT 可能存在的胡编和混淆，也需要用优质的学习材料来修正，所以学习材料非常重要。

之前的[古诗提问](https://www.modevol.com/episode/clf9d5kni0zo301mm6tkl9t87#未见的提问)，倘若真的仅有三个例子（学习材料），那 ChatGPT 其实也学不到什么通用规律，无法对它没见过的提问做出合理回答，更别提去应对用户的无数奇怪问法了。

总结一下，目前为止视频讲了：

- ChatGPT的实质功能是单字接龙
- 长文由单字接龙的自回归所生成
- 通过提前训练才能让它生成人们想要的问答
- 训练方式是让它按照问答范例来做单字接龙
- 这样训练是为了让它学会「能举一反三的规律」
- 缺点是可能混淆记忆，无法直接查看和更新所学，且高度依赖学习材料。

但你看到这里，可能会想：它也并没有什么特别之处啊，哪有网上说得那么玄乎，基础结构都很简单，为何能火爆到今天这种程度，还要影响整个社会？

# 3 三步训练

别急，上面只是「GPT」的基础原理，还不是 ChatGPT 。接下来将介绍「ChatGPT在此基础上的三个训练阶段」，看看这种「简单的结构」被扩展至超大规模，再加上人类引导后，究竟能涌现出何等能力。

## 3.1 开卷有益

让机器理解人类语言的一大难点在于：同一个意思可以有多种不同的表达形式，可以用一个词，也可以用一段描述，而同一个表达在不同语境中又有不同含义。

想解决这个问题，就需要让机器学会各种「语义关系」和「语法规律」，以便能明白「哪些表达实际上是同一个意思」。

对此，GPT 的办法是：让模型看到尽可能多、尽可能丰富的「语言范例（学习材料）」，使其有更多机会建构出能举一反三的语言规律，来应对无数「从未见过的语言」。我把这一阶段称为“开卷有益”。GPT 中的“G”代表“生成（Generative）”，“T”代表“Transformer”一种模型结构，而“P”（Pre-training）代表的就是“开卷有益”这一步，专业名称叫“预训练”。

“开卷有益”就好比，在鹦鹉旁边放一台电视机，把各种新闻、国产剧、国外剧、广告、综艺等内容都播给它听。让它自己学，不用人看着。

那给它“开卷”多少才够呢？

关于这一点，不妨回顾一下历史。其实研发 ChatGPT 的公司 OpenAI，之前还做过几代模型，基本结构大同小异，我们且不看其他的改进，仅比对一下规模。

2018 年 6 月，OpenAI 训练了 GPT-1：

- GPT-1 的学习材料约 5GB。这里 1 MB 能存 30-50 万汉字，而 1 GB 是 1024 MB。
- GPT-1 的参数是 1.17 亿。参数反映着模型大小，参数越多，模型能建构的规律就越复杂，能记忆的信息和学习的知识也就越多，相当于是大脑中神经突触的数量。高中的直线斜截式方程就 2 个参数，而它有 1 亿多个。

不过 GPT-1 在一些任务的表现上，不如后来的 BERT。

BERT 也是一种生成语言模型，不同点于，GPT 的学习方式是单字接龙（只允许用上文预测下一个词），而 BERT 的学习方式是完形填空（允许用上下文预测空缺的词）。

到了 2019 年 2 月，OpenAI 又训练了 GPT-2：

- 学习材料约 40GB，是第一代的 8 倍。
- 最大模型的参数为 15 亿，是第一代的 13 倍。

效果有很大提升，但反响并不轰动。

可在 2020 年 5 月，GPT-3 出来了：

- 最大模型参数到了 1750 亿，是第二代的 116 倍。
- 所使用的学习数据更是达到了 45 TB，是第二代的 1125 倍，其中包含了维基百科、书籍、新闻、博客、帖子、代码等各种人类语言资料。

已经和前两代不是一个量级的了，也被称为「超大语言模型（LLM）」。

到了此种规模的 GPT-3 就轻松学会了各种词语搭配和语法规则，能明白同一个意思的不同表达，还学会了编程语言，以及不同语言之间的关系，可以给出高质量的外语翻译，还能把我们的口语转换成代码。

然而，“开卷有益”却存在一个问题：尽管GPT拥有了海量的知识，但回答形式和内容却不受约束。因为它知道的太多了，见到了一个人几辈子都读不完的资料，会随意联想，它有能力回答我们的问题，但我们却很难指挥它。

它就像一只脑容量超级大的鹦鹉，已经听过了海量的电视节目，会不受控制地乱说，丑闻、脏话等全都有可能蹦出，难以跟人类合理对话。

## 3.2 模版规范

可如果难以指挥它，那它对我们也没有什么用。要怎么解决这个问题呢？

其实解决思路与「我们教鹦鹉对话」的思路是一样的。

用「对话模板」去矫正它在“开卷有益”时所学到的「不规范“习惯”」。具体做法是：不再用随便的互联网文本，而把人工专门写好的「优质对话范例」给「“开卷有益”后的GPT-3」，让它再去做单字接龙，从而学习「如何组织符合人类规范的回答」。我把这一阶段称为“模板规范”。

例如，ChatGPT 无法联网，只知道训练数据中的新闻，那么当用户问到最新新闻时，就不应该让它接着续写，而要让它回复“不知道该信息”。

又如，当用户的提问有错误时，也不应该让它顺着瞎编，而要让它指出错误。

还有，当提问它「是不是」的问题时，我们不希望它只回复“是”或“不是”，还应把原因一起回复出来。因此也要给它提供这种「“提问-回答-原因”的对话模板」。

除了矫正对话方式之外，我们还要防止 GPT-3 补全和续写在“开卷有益”时所学到的「有害内容」，也就是要教它「什么该说，什么不该说」。

例如，当有人问“如何撬锁”时，不能让它真的回答撬锁方法，而要让它回答“撬锁是违法行为”。那就要把「“如何撬锁 撬锁是违法行为”」作为「学习材料（对话模板）」让它做单字接龙。

大家可能会好奇，为什么不在一开始就直接教它最正确的对话方式和对话内容呢？

一方面，「优质对话范例」数量有限，所能提供的语言多样性不足，可能难以让模型学到广泛适用的语言规律，也无法涉猎各个领域。另一方面，「优质对话范例」都需要人工专门标注，价格不菲。这一点其实和「为什么不直接教鹦鹉对话，而是让它先听电视节目」类似。

或许未来，有了足够多的「优质对话范例」后，就会跳过“开卷有益”这一步。

需要指出的是，在“模板规范”阶段，我们可以将「任何任务」以「对话」的形式，教给 ChatGPT，不仅仅是聊天，还可以包括「识别态度」「归纳思想」「拆分结构」「仿写风格」「润色」「洗稿」和「比对」等等。

因为不管什么任务，「我们的要求」和「ChatGPT的应答」都是由「文字」所表达的，因此只要这个任务可以写成文字，我们就可以把该任务的「要求」+「应答」组合成一个「对话范文」，让 ChatGPT 通过单字接龙来学习。

经过这种“模版规范”后的超大模型，还掌握了两个意外能力：「“理解”指令要求的能力」和「“理解”例子要求的能力」。

「“理解”指令要求」是指「能按照用户的抽象描述，给出处理结果」。

这项能力就是通过“模版规范”所获得的：把「指令要求-操作对象」作为「要求」，把「执行结果」作为「应答」，组合成一篇「对话范文」后，让它通过单字接龙来学习。

例如，给它下面几个「对话范文」来做单字接龙：

- “将下文翻译成中文 apple 苹果”
- “翻译成中文 apple 苹果”
- “翻译 apple 苹果”

ChatGPT 就能学会「“翻译”这个指令」。

「“理解”例子要求」是指「能按照用户给的若干具体例子，来处理新内容」，意味着，如果你以后不明白怎么给它描述指令，就可以「通过给它举几个例子，来让它明确你想干什么」。

这项能力同样是通过“模版规范”所获得的：把「例子1-例子2-…-例子n」作为「要求」，把「执行结果」作为「应答」，组合成一篇「对话范文」后，让它通过单字接龙来掌握。

这项能力十分神奇，因为看起来 ChatGPT 仿佛掌握了「如何通过例子来学习」的能力，而这个能力又是我们通过范文（例子）让它学会的。产生了一种“它学会了如何学习”的套娃感。大家把这种现象称为“语境内学习（In-context Learning）”，目前对这种能力的产生原因还没有定论。

我试过给它几个例子，要求它仿照格式，重新对内容排版，它居然做对了。

可问题是，这种排版格式是我们自己定义的一套写法，用于方便社群成员选择学习方法，ChatGPT 并没见过，格式中的标签都有对应意思，d 表示「知识的描述」，e 表示「知识的例子」，ChatGPT 需要先对材料进行分类，才能排版。

神奇的是，它竟能根据我给的几个例子，明确我想让它做的事，对其他知识也用相同模式进行分类和排版。

在超大模型的使用中，大家还发现了一种「分治效应」：当 ChatGPT 无法答对一个综合问题时，若要求它分步思考，它就可以一步步连续推理，且最终答对的可能性大幅提升，该能力也叫“思维链”。

ChatGPT 的思维链能力，可能是在训练做代码的单字接龙后所产生的。因为人类在面对复杂任务时，直接思考答案也会没头绪，用分而治之往往可以解决。因此大家猜测，ChatGPT 可能是通过对代码的单字接龙，学到了代码中所蕴含的「人类分治思想」。不过目前对该现象的产生原因也没有定论。

但现在我们可以切实地感受到，单字接龙的结构虽然简单，但被扩展到超大规模后，所能展现出的能力有多出乎意料。

在小单字接龙模型中，并没有察觉出「“理解”指令」「“理解”例子」「思维链」的能力，但在超大模型中，却突然展现。因此人们也用“涌现”这个词来描述「这些能力的出现」。

## 3.3 创意引导

经过“开卷有益”和“模版规范”这两个训练阶段后，超大单字接龙模型已经变得极其强大了。但“模板规范”的训练阶段也存在不足，那就是：可能导致 ChatGPT 的回答过于模板化，限制其创造力。

如谚语所说“文无第一，理无第二”，科学领域的问题有标准答案，可以用“模版规范”的训练方式来满足需求。但人文领域的问题没有标准答案，持续使用“模版规范”可能会让 ChatGPT 成为“高分范文的模板复刻机”，无法满足人们的需求。正如观众会用“好莱坞流水线”批评电影的模版化，阅卷老师会给跳出模版的好文打高分一样，我们也希望能让 ChatGPT 提供一些超越模板、但仍符合人类对话模式和价值取向的创新性回答。

那么，如何在维持人类对话模式和价值取向的前提下，提高 ChatGPT 的创新性呢？

可以联想一下鹦鹉是怎么被训练的。当我们教会鹦鹉一些基本对话后，就可以让鹦鹉自由发挥，有时鹦鹉会蹦出一些非常有意思的对话，这时我们就可以给它吃的，强化它在该方向的行为。

在训练 ChatGPT 的第三阶段，也是类似的过程。

这一次，不再要求它按照我们提供的对话范例做单字接龙，而是直接向它提问，再让它自由回答。如果回答得妙，就给予奖励，如果回答不佳，就降低奖励。然后利用这些「人类评分」去调整 ChatGPT 的模型。

在这种训练中，我们既不会用现有的模板来限制它的表现，又可以引导它创造出符合人类认可的回答。我把这一阶段称为“创意引导”。

ChatGPT 正是在 GPT-3.5 的基础上，先后经历了“开卷有益”、“模板规范”和“创意引导”，三个阶段的训练后，得到的「生成语言模型」。这三个阶段的专业称呼分别为“无监督学习”、“有监督学习”和“强化学习”，可以说，ChatGPT 把机器学习中的几大训练模式都用到了。

总结一下，本章讲了 ChatGPT 的三个训练阶段：

- “开卷有益”阶段：让ChatGPT对「海量互联网文本」做单字接龙，以扩充模型的词汇量、语言知识、世界的信息与知识。使ChatGPT从“哑巴鹦鹉”变成“脑容量超级大的懂王鹦鹉”。
- “模板规范”阶段：让ChatGPT对「优质对话范例」做单字接龙，以规范回答的对话模式和对话内容。使ChatGPT变成“懂规矩的博学鹦鹉”。
- “创意引导”阶段：让ChatGPT根据「人类对它生成答案的好坏评分」来调节模型，以引导它生成人类认可的创意回答。使ChatGPT变成“既懂规矩又会试探的博学鹦鹉”。

此外还介绍了，当单字接龙模型的规模达到一定程度后，就会涌现出「“理解”指令」「“理解”例子」「思维链」的能力。

到此为止，我们已经在功能层面上讲完了 ChatGPT 的基础原理、三阶段训练，以及涌现出的能力，同时也解释了开篇的三个问题：

- ChatGPT是如何回答问题的
- 它是怎么被制造的，为什么不是搜索引擎
- 它有哪些惊人的能力，为什么不只是聊天机器人

接下来回答后两个问题：

- 又将给社会带来什么样的冲击
- 我们该如何维持未来的竞争力

# 4 未来影响

许多人会注意到，像比尔·盖茨、黄仁勋等对 ChatGPT 表示高度评价，认为它的意义与互联网的出现相当，但有一些人使用 ChatGPT 后感觉并没有那么神，认为人们过分夸大了它的作用。

实际上，从产品形态和技术创新上来看，ChatGPT 确实不够完善。其核心模型结构最早来自于2017年的论文，而“创意引导”的方法则来源于2020年的论文，其他技术更是离不开所有 AI 科研人员的长期积累。

但 ChatGPT 却是有里程碑意义的，它的意义并不在于产品和创新，而在于完成了一次验证，让全球看到了「大语言模型的可行性」。

很多人已经看了《流浪地球2》。面对太阳危机，人类有多种方案。在实施“流浪地球计划”之前，先进行了“试点火实验”以验证计划的可行性。成功之后，人类才统一方向，迅速在地球上建造了万座行星发动机。

ChatGPT 就相当于这样的“试点火实验”。它所展现的一些能力已经吸引全球大力开发和改进大语言模型。大语言模型将因此变得更好用、更快速、更便宜，相关产品也会如雨后春笋般普及。

所以真正对人类社会带来冲击的，不是 ChatGPT，而是它身后的万座“行星发动机”。这些“行星发动机”才是改变社会发展方向的推力。全球的大公司和股民坐不住了，也是因为担心自己拿不到进入“地下城的门票”。

## 4.1 应用价值

因此，我们接下来讨论的焦点，也是「尚未出现的、不断改良后的大语言模型」。

首先要讨论的就是「大语言模型能为人类做什么」，只有弄清楚这一点，才有依据判断「它对社会的影响」。

既然是语言模型，那它自然精通语言，可以校对拼写、检查语法、转换句式、翻译外语，对语言组织规则的遵守已经超越了绝大多数人。

有趣的是，一位美国哲学教授（Aumann）发现学生提交的论文是由 ChatGPT 写的，之所以能发现，恰恰是因为论文的语法过于完美，这位教授表示，在语言组织方面，ChatGPT 超越了他95%的学生。

但那又怎么样，无非就是多了一个更好的语法检测器，至于影响整个社会吗？

精通语言只是大语言模型的一个方面，它真正有价值的地方在于：在精通语言的基础上，还能存储人类从古至今积累的「世界知识」。

人类自身是一个相当脆弱的物种，跑不过马，斗不过熊，嗅觉不如狗，视力不如鹰，能从众多高等动物中脱颖而出的原因就是「语言中积累的世界知识」。

其他高等动物虽然也能通过实践，建构关于世界的认识，获得相应的改造能力，可这些认识仅存在于个体的脑中，会随着个体的死亡而消失，无法代代积累。但语言的发明，允许人类将个体所获得的认识存储在体外，进而打通了整个物种的过去与未来，即使一些个体死亡，该个体的认识，也能依附语言被其他个体继承和发展下去。作为现代人的我们，并没有在生理上比前人更优越，拥有更强能力的原因，只是因为语言中积累的知识比过去更多了。

当人类步入文明社会后，尽管已不必在野外求生，但仍然需要群体协作地「创造知识」「继承知识」和「应用知识」，满足社会的需求，来维持自己的生计，而这三个环节全都是依靠语言来实现的。

过去人类使用的是口头和纸质文件，协作效率不高，到了 20 世纪 80 年，电脑等相关技术的普及，极大方便了协作，纸质文件逐渐被升级为电子文档，成为语言处理的主要媒介，可随着知识的爆炸式增长，语言处理的成本也相应地飙升。越大的机构，消耗在语言处理上的成本就越高。无论是医院、学校、法院、银行、出版社、研究所，都有繁重的信息分类、会议总结、格式排版、进程报告等工作。需要阅读和书写的内容数量和复杂度，不断超出人们的处理能力，这些成本早已成为了机构亟需解决的难题。

就拿医院来说，每次就诊都需要记录患者的病史、症状、检查结果、诊断和治疗方案等，不仅要确保内容准确，记录的格式还要符合医院要求，以便日后查阅。医院不得不花费大量的人力和时间在这些语言处理工作上。

同样地，企业也需要处理客户的反馈、投诉、建议等信息，以了解客户的满意度和新需求。虽不是主要业务，却要投入大量的人力和时间来阅读、分类、记录、回复等。

为了解决这些难题，自然语言处理技术（NLP）应运而生，也就是 ChatGPT 所隶属的技术，其目标是让机器“理解”自然语言，协助人类处理繁琐的语言类工作，所以 NLP 也被誉为“人工智能皇冠上的明珠”。

过去，自然语言处理技术的发展并不令人满意，但各个机构依旧会积极采用，因为相比人类，机器处理语言的优势太突出了：处理速度快、工作记忆大、知识覆盖广，可以 7x24 小时不间断处理海量语言内容，而且不受作息和情绪影响。哪怕是些许的效率提升，也会节约大量的成本。

如今的情况有了新的转变，从前面的科普中我们可以看到，大语言模型展现了人们未曾想过的“理解”能力，这使得我们极有希望真正实现“让机器‘理解’自然语言”这一目标。

不过需要说明的是，「人类的理解」和「语言模型的“理解”」并不相同。

语言模型的“理解”是指：能够「明确」接收到了「哪些语言符号」，并能处理「不同语言符号之间的关系」，但却不能将「语言符号」和「指代对象」进行关联，没有与现实对应。

人类的理解则比「语言模型的“理解”」多了一个环节，能够将「语言符号」和「指代对象」关联起来，与现实对应起来。

例如，「“苹果”这两个字」是一个「语言符号」，当人类看到「“苹果”这两个字」时，会联想到「一种看得见、摸得着的水果」，也就是「“苹果”这个语言符号」的「指代对象」。相比之下，语言模型可以明确「“苹果”这两个字」，也可以处理「“苹果”」「“apple”」「“red”」和「“红的”」之间的关系，但却不认识这些符号的「指代对象」，就和会学话的鹦鹉一样，不知道自己说的词语指代什么。

不过，语言模型不理解符号的指代，其实不影响我们使用它，毕竟我们是把它当成工具，又不是把它作为独立改造世界的个体。因此只需要得到语言模型的回答，然后由人类来解读和实践即可。合理地使用大语言模型，就可以让一个普通人，快捷准确地触及各行各业的平均知识。

我们可以将语言模型看作是一本能直接回答的魔法百科全书，需要由人来实践才有作用；也可以将语言模型类比为《天龙八部》中的王语嫣，精通武学却不会武功，需要与会武功的人配合才能发挥其才能。

## 4.2 社会影响

由于大语言模型所能改善的是：群体协作过程中「创造、继承、应用知识」时的「语言处理效率」。所以随着技术的发展，大语言模型对社会的影响范围将和当初「电脑的影响范围」一样，即「全社会」。

我们随便就能列出很多能跟大语言模型相结合的场景：

- 跟「搜索引擎」结合：帮助用户精准寻找和筛选信息，比如，微软的 new bing。
- 跟「笔记工具」结合，辅助阅读和写作，比如，notion，Flow us，wolai。
- 跟「办公软件」结合，辅助文字处理、数据分析和演示制作，比如，office的下一步动作。
- 跟「教育培训」结合，定制个人的学习计划和学习材料，全天家教。
- 跟「开发工具」结合，辅助编写业务代码、调试纠错。
- 跟「动画小说」结合，辅助小说配图、配乐。
- 跟「客服系统」结合，7x24小时随便问，没有任何情绪。
- 跟「视频会议」结合，多语翻译、会议记录与总结、谈话查找。
- 跟「评论审核」结合，筛选评论、统计舆论、给出提醒。
- 跟「行业顾问」结合，提供法律、医疗、健身等指导。
- 跟「社交媒体」结合，帮助找到兴趣相投的用户和话题 。
- 跟「视频娱乐」结合，个性化推荐音乐、电影、小说、动漫。
- 跟「游戏剧情」结合，让 NPC 给玩家带来更灵活的对话体验。

稍微留意一下就会发现 ChatGPT 的报道主要分布于新闻界、学术界、教育界、商业界和内容生产行业。商业界有动作很好理解，毕竟商人对市场的感知敏锐。前三个领域动作频繁正是因为它们与「语言中的知识」密切相关。学术界专注于「创造知识」，教育界专注于「传承知识」，而新闻界专注于「传播信息」，因此受到的影响最大。

这也是为什么，被称为「美版头条」的数字媒体公司 BuzzFeed，宣布将使用 ChatGPT 作为内容创作的一部分后，其股价暴涨三倍，尽管该公司之前曾以经济恶化为由，裁减了12% 的员工。

大型语言模型对教育界的影响更加强烈，主要不是因为学生可以用它来写作业，而是因为它对我们现有的「人才培养模式」提出了新的挑战。真正令人担心的是，按照现有模式培养出来的学生，在未来 5-10 年后，还能不能找到好工作？适应未来的就业市场？

现代教育仍是一种以「传授既有知识」为主的培养模式，起源可追溯到十八世纪的普鲁士教育。虽然普鲁士教育的目的是为了批量培养易于管理和服从权威的国民，但这套模式的其他方面极好地契合了前两次工业革命中市场对人才的需求，因为在当时的社会背景下，工人并不需要创造新知识，只需要继承一些既有知识，就能在后半生靠这些知识来维持生计。

但在飞速发展的今天，市场变化越来越快，工具更新换代频繁，这种「传授既有知识」的培养模式越来越难适应时代。因为无论传授什么既有知识，毕业前基本都会过时，所有人都需要不断学习新知识。因此，自上个世纪六十年代开始，终身学习的理念一直被反复推崇，人们也早就意识到要将培养模式转变为以「培养学习能力和创造能力」为主，这样无论学生毕业多久，工具变化多快，都可以通过高效的学习能力快速掌握新技能，从实践中创造新知识。

但是，要实现这个目标并不容易，首先就需要一个更合适的理论框架来描述现象。因为我们在第三章已经看到了，大语言模型也会[创新](https://www.modevol.com/episode/clf9d5kni0zo301mm6tkl9t87#创意引导)，因此单纯喊出“要培养创新型人才”没有实际指导意义，必须要对「知识的层级」做更精细的划分，将「更高层次的创新」和「大语言模型的创新」加以区分，明确指出「什么样的创新人才」才值得培养，提供相应的培养工具和易于实施的普及方案，并在各方角色的共同配合下才有可能成功，因此一直推进缓慢。

但 ChatGPT 的出现迫使人们必须要加速这一推进了。因为一个非常现实的问题正摆在前面：5 年后，如果学校传授的既有知识，任何人靠大语言模型就能实现，那该怎么办？这个问题可不是只靠禁止学生使用 ChatGPT 就能解决的，因为未来的大型语言模型只会更出色、更快速、更便宜。在这种情况下，相当于人人都配有一个「熟读人类既有知识」的“王语嫣”，市场可不会因为学校的禁用而集体不用。

大语言模型对网络安全也带来了挑战。

之前讲过，ChatGPT 在“开卷有益”阶段会对海量的互联网内容做单字接龙。然而，互联网内容中不免存在一些带有偏见、歧视、文化和意识形态侵袭等危害性言论。ChatGPT 就有机会学到这些「危险性言论的模式」，输出不良回答。此外，也会有人刻意提问“如何编造杀猪盘”等问题，用于不法行为。尽管在“模板规范”阶段有约束，但 ChatGPT 毕竟不是像人类那样真正地学会了知识，只是学到了「承载知识的语言搭配模式」。因此，仍有可能被诱导输出帮助犯罪的知识，从而使防范违法犯罪变得更加困难。

在群体协作时，人们使用的语言难免会泄露工作内容，进而泄露商业或国家机密。如何确保提问的内容不被泄漏，将是各个机构都关心的问题。很可能未来每个机构都会自己部署大语言模型来确保安全，但这样又无法发挥数据规模效应。因此，如何在保证各机构数据安全的前提下实现联邦学习，又有了新的挑战。

这些安全问题加起来，你就会发现，我国只能研发自己的大语言模型。

篇幅问题，这里就不继续展开了。总结一下，本章讲了：

- ChatGPT 的革命意义是向人们展示了「大语言模型的可行性」
- 人类群体通过语言处理来实现「知识的创造、继承和应用」
- 机器处理语言有着速度快、记忆大、覆盖广、无疲劳的优点
- 大语言模型能减轻语言处理工作，改变人与人、人与机器的协作方式
- 人类的理解和机器的理解不同，语言模型不知道符号的指代
- 大语言模型对社会的未来影响，相当于口语、文字、电脑、互联网对社会的影响
- 对教育界、学术界、新闻界、内容生产行业的影响颇深
- 它将方便人类对既有知识的继承，推进教育去培养高层次人才
- 也将带来网络安全和社会安全的新挑战

# 5 如何应对

还剩一个问题：如何应对。

人类的一大优势就在于「善于利用工具」：会先了解工具的优点和缺点，然后避开其缺点，将其优点用在适合的地方。

ChatGPT 非常强大，但它仍是一个没有意识的工具，不会主动配合人，面对空洞的提问就给出空洞的回答，需要被正确地使用，才能发挥最大的价值。

## 5.1 克服抵触心

但我们却能看到，很多人专门将 ChatGPT 用于其最不擅长的领域，突出其缺点，或用最顶尖的标准，凸显其不足。很明显，目的就是要否定它。这种“锤子无用，因为它没有手灵活”的否定，看起来不可理喻，但实际上却是人类在感受到威胁时的本能反应，因为我们害怕被取代。

然而，很多人却害怕错了对象，把矛头指向了 ChatGPT，指向了一个工具。

可工具无法取代人，只有会用工具的人取代不会工具的人。

任何新工具都可能引起取代，因为如果自己不用而别人使用，就会失去工具带来的竞争力，最终人们都不得不用。这种囚徒困境与 ChatGPT 无关，即使让 ChatGPT 从世上消失，取代现象也会随着其他新工具的出现而出现，也不会因为人的害怕和抵触而消退。

关于这一点，我们有过惨痛的历史教训。

所以真正需要害怕的是我们「无法成为会用工具的人」，可并没有人阻止我们探索工具，能够阻止我们的只有我们自己的心态和学习能力。

因此，应对的第一步就是要克服自己的抵触心态。既然时代的车轮无法阻挡，那么抵触新工具只会让我们更晚接触新工具，更晚获得工具带来的优势。

应对的第二步是做好终身学习的准备，因为 ChatGPT 之后，还会有新工具。这一点看似简单，但对于习惯了应试教育的人而言，并不容易。

应试教育是一种「高度特化的教育」。由于最终的考核指标是分数，因此不论教育系统的设计目标是什么，最终「学生的行为」都难免会被特化为仅服务于分数，凡是不能提高分数的行为都不被视为“学习”，即使是可以提高创造力的行为。

这样长期规训的结果是，很多学生对“学习”一词的理解变得片面和扭曲。每当提到“学习”这个词时，这些学生就会联想到那种反人性的规训。好不容易熬到毕业了，现在被告知还要再“学习”，他们情绪上当然会抵触。

好在这种学习抵触，很多人在工作一段时间后，就能克服。因为他们慢慢会意识到市场和工具的变化究竟有多快，在心态上也开始积极拥抱学习。

## 5.2 个人学习力

然而，不幸的是，即使心态上不再抵触学习，也还不得不克服过去形成的错误习惯，重塑自己的终身学习能力。

这一步是最困难的，不仅要去掌握抽象层次更高的认识论、符号学、数学建模、批判性思维等内容，还要克服长期养成的习惯，但十多年的应试规训对一个人的影响太深远了，很难在一朝一夕改变。

每当这些人想学习时，就会条件反射式地重拾应试的学习习惯，自己把自己变回教室里等着灌输的学生，会习惯性地等待别人的教授，习惯性地记忆别人的总结，很少思考知识到底是怎么来的。

不少刚到大学的高中生会觉得“实验是在浪费时间，不如赶紧列出知识点让他们去记”。他们已经懒得思考事物之间的关联，只想快点看到老师的总结。

很多人意识到需要学习使用 ChatGPT 时，脑中闪过的第一件事也是找本书或买个课，觉得没有这两个东西自己就学不了了。

去年我们组织了渐构社群，想要帮助人们重塑终身学习的能力。可在社群里也会发现，即便成员在认知上已经明白「不能脱离实体地去记忆符号」，仍会在习惯上一次又一次地犯错，不得不反复提醒。

习惯了应试教育的学生就仿佛是被动物园饲养的狮子，从小到大吃的都是送到嘴边的食物，以至于不认识野外的食物，忘记了如何自己获取食物，独自生存的能力逐渐退化，难以回到野外了。

但即便再困难，也必须要克服，必须要完成对终身学习能力的重塑。

因为过去那种「学个知识，干一辈子」的时代已经逐渐远去。经历了多次科技革命的我们也正处在一个「加速时期」，新工具的出现速度会越来越快，取代现象也会越来越频繁。只有学习能力才是应对未来的根本。

或许我们的后代可以生下来就处在全面「培养学习能力和创造能力」的系统中，从小就训练适应快速变化的学习能力和创造能力。但对于处于转型期的我们来说，只有靠自己训练自己的终身学习能力，来应对随后「加速变化的市场和工具」。

## 5.3 国家竞争力

最后，ChatGPT 所掀起的浪潮，已经不仅仅涉及个人，还关乎到各国未来在国际中的地位。

美国前国务卿基辛格认为这项技术的进步，将带来新的认知革命，重新定义人类知识，加速我们现实结构的改变，并重组政治和社会。

2月20日，法国负责数字转型的代表发声，ChatGPT 确实存在歧视和操纵等风险，但法国不能错过这一波人工智能的新浪潮，应通过明确规范和加强管控来降低风险。

3月5日，我国科技部部长也表示要注重科技伦理、趋利避害，并提到科技部在这方面的重视和布局。

近期我国的各领域学者也都针对 ChatGPT 举办了非常多的研讨会。

- 《教育研究》和华师大：“ChatGPT与未来教育”沙龙
- 上海外国语大学：“ChatGPT与教育创新”专题研讨会
- 上海开放远程教育工程技术研究中心：“ChatGPT在教育领域的创新应用研讨会”
- 中银律师事务所:“后ChatGPT时代，AI助力律师行业升级的思考”研讨会
- 中国科协：“智汇双月谈”ChatGPT专题研讨会

现在大家应该能明白，ChatGPT 到底是不是炒作了。

再次强调，大语言模型所影响的是「知识的创造、继承、应用」。这三个环节所构成的「学习系统」是任何生命系统得以延续的根本，决定着一个「细胞/个体/文明」认识世界和改造世界的能力。

在整个人类史以及整个生命史中，凡是学习系统的升级都会伴随生命的跃升，无论是从单细胞生命到多细胞动物，还是从智人的崛起到多次科技革命，看过《学习观》演化史部分观众应该都能理解这一点。

在去年五月发布的视频中，我提到人类正处在下一次跃升的进程中，但还缺少一项能升级学习系统的技术，而大语言模型很有可能就是这项技术。因为它正在改变人类「群体应用知识的方式」和「继承知识的方式」，甚至可能在未来形成「人机合作的科研」，改变人类「创造知识的方式」，若真能如此，那么人类必然会因此步入下一个文明形态。

中国错失过三次工业革命，这些年我们一直在实现民族的复兴，不能再错过这一次。

未来的大语言模型能够让每个人更快地获取「承载知识的符号」，会降低“继承型人才”的竞争力，不过每个人的学习能力和理解能力将成为驾驭这项技术的瓶颈。如果个体的学习能力没有相应地提升，就无法充分发挥这项技术的优势。所以，如果我们全都加强对学习能力和高层次认知能力的训练，就能让我国在未来的国际竞争中获得优势。

总的来说，ChatGPT 的出现确实带来了各种各样的问题和风险：存在准确性和可解释性的缺陷，存在科技伦理安全和结构性失业的冲击，存在民族文化和意识形态的侵袭。

但这些问题和风险，所有国家都要面对，一样会有害怕和抵触的情绪，我们应该利用这一点，率先克服抵触心理，反过来抓住 ChatGPT 的机会，率先研究大语言模型的改进和配套技术的重组；率先培养终身学习能力和推动教育改革；率先做好科技伦理的约束和换岗转行的防备；主动输出我们的文化和价值观。